services:
  # Zookeeper (required for Kafka)
  zookeeper:
    image: confluentinc/cp-zookeeper:7.3.0
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"
    networks:
      - kafka-net

  kafka:
    image: confluentinc/cp-kafka:7.5.0
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "false" # Отключаем автосоздание
    ports:
      - "29092:29092"
      - "9092:9092"
    networks:
      - kafka-net

  # Topic creation service
  kafka-init:
    image: confluentinc/cp-kafka:7.5.0
    depends_on:
      - kafka
    command: >
      bash -c "
        echo 'Waiting for Kafka to be ready ...'
        cub kafka-ready -b kafka:9092 1 30
        echo 'Creating topics ...'
        kafka-topics --create --if-not-exists --topic page-views --bootstrap-server kafka:9092 --partitions 3 --replication-factor 1
        kafka-topics --create --if-not-exists --topic cart-events --bootstrap-server kafka:9092 --partitions 3 --replication-factor 1  
        kafka-topics --create --if-not-exists --topic order-events --bootstrap-server kafka:9092 --partitions 3 --replication-factor 1
        echo 'All topics created'
      "
    networks:
      - kafka-net

  event-generator-view:
    build:
      context: .
      dockerfile: ./generators/view/Dockerfile
    environment:
      KAFKA_BROKERS: "kafka:9092"
      TOPIC: "page-views"
    depends_on:
      - kafka
      - kafka-init
    restart: unless-stopped
    networks:
      - kafka-net

  event-generator-cart:
    build:
      context: .
      dockerfile: ./generators/cart/Dockerfile
    environment:
      KAFKA_BROKERS: "kafka:9092"
      TOPIC: "cart-events"
    depends_on:
      - kafka
      - kafka-init
    restart: unless-stopped
    networks:
      - kafka-net

  event-generator-order:
    build:
      context: .
      dockerfile: ./generators/order/Dockerfile
    environment:
      KAFKA_BROKERS: "kafka:9092"
      TOPIC: "order-events"
    depends_on:
      - kafka
      - kafka-init
    restart: unless-stopped
    networks:
      - kafka-net

  # Kafdrop - Web UI for viewing Kafka topics
  kafdrop:
    image: obsidiandynamics/kafdrop:3.30.0
    environment:
      KAFKA_BROKERCONNECT: "kafka:9092"
      JVM_OPTS: "-Xms32M -Xmx64M"
    ports:
      - "9002:9000"
    depends_on:
      - kafka
      - kafka-init
    networks:
      - kafka-net

  clickhouse:
    image: clickhouse/clickhouse-server:23.8
    ports:
      - "8123:8123" # HTTP interface
      - "9000:9000" # Native protocol
    environment:
      CLICKHOUSE_DB: "ecommerce"
      CLICKHOUSE_USER: "admin"
      CLICKHOUSE_PASSWORD: "password"
      CLICKHOUSE_DEFAULT_ACCESS_MANAGEMENT: 1
    volumes:
      - clickhouse_data:/var/lib/clickhouse
      - ./clickhouse/init:/docker-entrypoint-initdb.d
    depends_on:
      - kafka
      - kafka-init
    networks:
      - kafka-net
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:8123/ping"]
      interval: 5s
      timeout: 10s
      retries: 3

  victoriametrics:
    image: victoriametrics/victoria-metrics:latest
    ports:
      - "8428:8428"
    command:
      - -storageDataPath=/victoria-metrics-data
      - -httpListenAddr=:8428
    volumes:
      - victoriametrics_data:/victoria-metrics-data
    networks:
      - kafka-net

  grafana:
    image: grafana/grafana:9.5.0
    environment:
      GF_SECURITY_ADMIN_PASSWORD: admin
    ports:
      - "3000:3000"
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana/provisioning:/etc/grafana/provisioning
    depends_on:
      - victoriametrics
    networks:
      - kafka-net

  # for AirFlow
  postgres:
    image: postgres:13
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - kafka-net

  # for AirFlow AirFlow
  redis:
    image: redis:6.2-alpine
    networks:
      - kafka-net

  airflow-init:
    image: apache/airflow:2.7.0
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow
      AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/scripts:/opt/airflow/scripts
    command: >
      bash -c "
        echo 'Waiting for PostgreSQL...'
        until nc -z postgres 5432; do sleep 1; done
        echo 'Initializing AirFlow database...'
        airflow db init
        echo 'Creating admin user...'
        airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com
        echo 'AirFlow initialization completed!'
      "
    depends_on:
      - postgres
      - redis
    networks:
      - kafka-net

  # AirFlow
  airflow-webserver:
    image: apache/airflow:2.7.0
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow
      AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: "false"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/scripts:/opt/airflow/scripts
      - ./airflow/requirements.txt:/opt/airflow/requirements.txt
    ports:
      - "8080:8080"
    command: >
      bash -c "
        pip install -r /opt/airflow/requirements.txt &&
        airflow webserver
      "
    depends_on:
      - postgres
      - redis
      - airflow-init
    networks:
      - kafka-net

  airflow-scheduler:
    image: apache/airflow:2.7.0
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow
      AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/scripts:/opt/airflow/scripts
      - ./airflow/requirements.txt:/opt/airflow/requirements.txt
    command: >
      bash -c "
        pip install -r /opt/airflow/requirements.txt &&
        airflow scheduler
      "
    depends_on:
      - postgres
      - redis
      - airflow-init
    networks:
      - kafka-net

volumes:
  clickhouse_data:
  victoriametrics_data:
  postgres_data:
  grafana_data:

networks:
  kafka-net:
    driver: bridge
